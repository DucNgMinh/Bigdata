{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import StructType,StructField, StringType, LongType, DateType\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.config(\"spark.driver.memory\", \"8g\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'C:\\Users\\ADMIN\\PycharmProjects\\Bigdata-1\\Data\\Dataset\\Clean_data'\n",
    "list_file = os.listdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Contract: string (nullable = true)\n",
      " |-- Type: string (nullable = false)\n",
      " |-- Giải Trí: long (nullable = true)\n",
      " |-- Phim Truyện: long (nullable = true)\n",
      " |-- Thiếu Nhi: long (nullable = true)\n",
      " |-- Thể Thao: long (nullable = true)\n",
      " |-- Truyền Hình: long (nullable = true)\n",
      " |-- Date: date (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emptyRDD = spark.sparkContext.emptyRDD()\n",
    "\n",
    "schema = StructType([\n",
    "    StructField('Contract', StringType(), True), \n",
    "    StructField('Type', StringType(), False), \n",
    "    StructField('Giải Trí', LongType(), True), \n",
    "    StructField('Phim Truyện', LongType(), True), \n",
    "    StructField('Thiếu Nhi', LongType(), True), \n",
    "    StructField('Thể Thao', LongType(), True), \n",
    "    StructField('Truyền Hình', LongType(), True), \n",
    "    StructField('Date', DateType(), False)])\n",
    "\n",
    "monthly_df = spark.createDataFrame(emptyRDD,schema)\n",
    "monthly_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(list_file)):\n",
    "    df = spark.read.csv(path + '\\\\' +  list_file[i], header= True)\n",
    "    monthly_df = monthly_df.union(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+--------+-----------+---------+--------+-----------+----------+\n",
      "| Contract|       Type|Giải Trí|Phim Truyện|Thiếu Nhi|Thể Thao|Truyền Hình|      Date|\n",
      "+---------+-----------+--------+-----------+---------+--------+-----------+----------+\n",
      "|DNH014998|Phim Truyện|    null|     3365.0|     null|    null|       null|2022-04-01|\n",
      "|HND486882|Phim Truyện|    null|     5545.0|     null|    null|       null|2022-04-01|\n",
      "|HUFD07189|Truyền Hình|    null|       null|     null|    null|     2264.0|2022-04-01|\n",
      "|HDFD36288|Truyền Hình|    null|       null|     null|    null|    11904.0|2022-04-01|\n",
      "|CTFD04401|Truyền Hình|    null|       null|     null|    null|    55881.0|2022-04-01|\n",
      "|HNH954607|Phim Truyện|    null|    13115.0|     null|    null|       null|2022-04-01|\n",
      "|HNH855959|Truyền Hình|    null|       null|     null|    null|      327.0|2022-04-01|\n",
      "|SGH034683|Truyền Hình|    null|       null|     null|    null|    82195.0|2022-04-01|\n",
      "|NTFD35330|Truyền Hình|    null|       null|     null|    null|    19139.0|2022-04-01|\n",
      "|NTFD48198|Phim Truyện|    null|    55202.0|     null|    null|       null|2022-04-01|\n",
      "+---------+-----------+--------+-----------+---------+--------+-----------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "monthly_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "month_summary = monthly_df.groupBy('Contract').agg(sum('Giải Trí').alias('Giai_Tri'),\n",
    "                                   sum('Phim Truyện').alias('Phim_Truyen'),\n",
    "                                   sum('Thiếu Nhi').alias('Thieu_Nhi'),\n",
    "                                   sum('Thể Thao').alias('The_Thao'),\n",
    "                                   sum('Truyền Hình').alias('Truyen_Hinh'),\n",
    "                                   max('Date').alias('Recent'),\n",
    "                                   countDistinct('Date').alias('Frequency'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1920545"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "month_summary.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------+-----------+---------+--------+-----------+----------+---------+\n",
      "|      Contract|Giai_Tri|Phim_Truyen|Thieu_Nhi|The_Thao|Truyen_Hinh|    Recent|Frequency|\n",
      "+--------------+--------+-----------+---------+--------+-----------+----------+---------+\n",
      "|113.182.209.48|    89.0|       null|     null|    null|       63.0|2022-04-01|        1|\n",
      "|14.182.110.125|    92.0|       null|     null|    null|      404.0|2022-04-10|        1|\n",
      "|     AGAAA0338|    null|       null|     null|    null|   278633.0|2022-04-30|       30|\n",
      "|     AGAAA0342|   204.0|       null|     null|    null|   117788.0|2022-04-30|       12|\n",
      "|     AGAAA0346|    null|       null|     null|    null|  2056249.0|2022-04-30|       30|\n",
      "|     AGAAA0353|    null|     1665.0|     null|    null|    25982.0|2022-04-30|       29|\n",
      "|     AGAAA0372|    null|       null|     null|    null|    13123.0|2022-04-30|       27|\n",
      "|     AGAAA0391|   373.0|      129.0|     null|    null|   158931.0|2022-04-30|       11|\n",
      "|     AGAAA0452|    null|       null|     null|    null|    29921.0|2022-04-30|       14|\n",
      "|     AGAAA0504|    97.0|       46.0|     null|    null|    21313.0|2022-04-30|       28|\n",
      "+--------------+--------+-----------+---------+--------+-----------+----------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "month_summary.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Contract: string (nullable = true)\n",
      " |-- Giai_Tri: double (nullable = true)\n",
      " |-- Phim_Truyen: double (nullable = true)\n",
      " |-- Thieu_Nhi: double (nullable = true)\n",
      " |-- The_Thao: double (nullable = true)\n",
      " |-- Truyen_Hinh: double (nullable = true)\n",
      " |-- Recent: string (nullable = true)\n",
      " |-- Frequency: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "month_summary.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "month_summary = month_summary.withColumn('Recent', to_date(col('Recent'), 'yyyy-MM-dd'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "month_summary = month_summary.withColumn('Diff_Day', datediff(to_date(lit('2022-05-01')), month_summary['Recent']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "month_summary = month_summary.withColumn('Recent_Score', when(col('Diff_Day') <= 3, 4)\n",
    "                                        .when((col('Diff_Day') >= 4) & (col('Diff_Day') <= 7), 3)\n",
    "                                        .when((col('Diff_Day') >= 7) & (col('Diff_Day') <= 14), 2)\n",
    "                                        .otherwise(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "month_summary = month_summary.withColumn('Frequency_Score', when(col('Frequency') <= 4, 1)\n",
    "                                        .when((col('Frequency') >= 5) & (col('Frequency') <= 12), 2)\n",
    "                                        .when((col('Frequency') >= 12) & (col('Frequency') <= 20), 3)\n",
    "                                        .otherwise(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "month_summary = month_summary.withColumn('RF_Score', concat_ws(\"\", month_summary['Recent_Score'], month_summary['Frequency_Score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_segment = month_summary.groupBy('RF_Score').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_segment.repartition(1).write.csv(r\"C:\\\\Users\\\\ADMIN\\\\PycharmProjects\\\\Bigdata-1\\\\Data\\Dataset\\\\Customer_segment_data\\\\\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "customer_segment = customer_segment.withColumn('total', sum('count').over(Window.partitionBy())).withColumn('percent', (col('count') / col('total')) * 100).orderBy('percent', ascending= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+-------+-------------------+\n",
      "|RF_Score|  count|  total|            percent|\n",
      "+--------+-------+-------+-------------------+\n",
      "|      44|1171794|1920545|   61.0136185301568|\n",
      "|      43| 204675|1920545| 10.657131178910154|\n",
      "|      42| 162209|1920545|  8.445987987784717|\n",
      "|      41| 142813|1920545|  7.436066324923394|\n",
      "|      32|  55286|1920545| 2.8786620464503567|\n",
      "|      21|  53195|1920545| 2.7697867011707613|\n",
      "|      31|  41733|1920545| 2.1729769414411013|\n",
      "|      22|  33113|1920545| 1.7241460106376054|\n",
      "|      33|  28436|1920545| 1.4806213861169615|\n",
      "|      34|  14218|1920545| 0.7403106930584807|\n",
      "|      23|  11167|1920545| 0.5814495364597028|\n",
      "|      24|   1906|1920545|0.09924266288996093|\n",
      "+--------+-------+-------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customer_segment.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = monthly_df.groupBy('Contract').agg(sum('Giải Trí').alias('Giai_Tri'),\n",
    "                                   sum('Phim Truyện').alias('Phim_Truyen'),\n",
    "                                   sum('Thiếu Nhi').alias('Thieu_Nhi'),\n",
    "                                   sum('Thể Thao').alias('The_Thao'),\n",
    "                                   sum('Truyền Hình').alias('Truyen_Hinh'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.na.fill(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-----------+---------+--------+-----------+\n",
      "| Contract|Giai_Tri|Phim_Truyen|Thieu_Nhi|The_Thao|Truyen_Hinh|\n",
      "+---------+--------+-----------+---------+--------+-----------+\n",
      "|HTFD11598|     0.0|    15551.0|      0.0|     0.0|    42919.0|\n",
      "|HPFD48556|    69.0|        0.0|      0.0|     0.0|  1468328.0|\n",
      "|NBFD10014|     0.0|        0.0|      0.0|     0.0|  1596494.0|\n",
      "|HNH619088|     0.0|    77275.0|  11361.0|     0.0|   917930.0|\n",
      "|HNH036174|     0.0|    62674.0|      0.0|     0.0|   354879.0|\n",
      "|DNH067877|     0.0|        0.0|      0.0|     0.0|   181308.0|\n",
      "|SGH806190|     0.0|        0.0|      0.0|     0.0|   217779.0|\n",
      "|HNH582022|     0.0|        0.0|      0.0|     0.0|  2209949.0|\n",
      "|HNH795510|     0.0|    30197.0|    265.0|     0.0|  1196936.0|\n",
      "|DNFD91557|     0.0|        0.0|      0.0|     0.0|    95567.0|\n",
      "+---------+--------+-----------+---------+--------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = r\"C:\\\\Users\\\\ADMIN\\\\PycharmProjects\\\\Bigdata-1\\\\Data\\Dataset\\\\Clean_data_1\\\\\"\n",
    "df.repartition(1).write.csv(save_path, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assemble=VectorAssembler(inputCols=['Giai_Tri', 'Phim_Truyen', 'Thieu_Nhi', 'The_Thao', 'Truyen_Hinh'], outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembled_data = assemble.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-----------+---------+--------+-----------+--------------------+\n",
      "| Contract|Giai_Tri|Phim_Truyen|Thieu_Nhi|The_Thao|Truyen_Hinh|            features|\n",
      "+---------+--------+-----------+---------+--------+-----------+--------------------+\n",
      "|HTFD11598|     0.0|    15551.0|      0.0|     0.0|    42919.0|(5,[1,4],[15551.0...|\n",
      "|HPFD48556|    69.0|        0.0|      0.0|     0.0|  1468328.0|(5,[0,4],[69.0,14...|\n",
      "|NBFD10014|     0.0|        0.0|      0.0|     0.0|  1596494.0| (5,[4],[1596494.0])|\n",
      "|HNH619088|     0.0|    77275.0|  11361.0|     0.0|   917930.0|[0.0,77275.0,1136...|\n",
      "|HNH036174|     0.0|    62674.0|      0.0|     0.0|   354879.0|(5,[1,4],[62674.0...|\n",
      "|DNH067877|     0.0|        0.0|      0.0|     0.0|   181308.0|  (5,[4],[181308.0])|\n",
      "|SGH806190|     0.0|        0.0|      0.0|     0.0|   217779.0|  (5,[4],[217779.0])|\n",
      "|HNH582022|     0.0|        0.0|      0.0|     0.0|  2209949.0| (5,[4],[2209949.0])|\n",
      "|HNH795510|     0.0|    30197.0|    265.0|     0.0|  1196936.0|[0.0,30197.0,265....|\n",
      "|DNFD91557|     0.0|        0.0|      0.0|     0.0|    95567.0|   (5,[4],[95567.0])|\n",
      "+---------+--------+-----------+---------+--------+-----------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "assembled_data.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "scale = StandardScaler(inputCol='features', outputCol='standardized')\n",
    "data_scale = scale.fit(assembled_data)\n",
    "data_scale_output = data_scale.transform(assembled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-----------+---------+--------+-----------+--------------------+--------------------+\n",
      "| Contract|Giai_Tri|Phim_Truyen|Thieu_Nhi|The_Thao|Truyen_Hinh|            features|        standardized|\n",
      "+---------+--------+-----------+---------+--------+-----------+--------------------+--------------------+\n",
      "|HTFD11598|     0.0|    15551.0|      0.0|     0.0|    42919.0|(5,[1,4],[15551.0...|(5,[1,4],[0.01469...|\n",
      "|HPFD48556|    69.0|        0.0|      0.0|     0.0|  1468328.0|(5,[0,4],[69.0,14...|(5,[0,4],[0.00809...|\n",
      "|NBFD10014|     0.0|        0.0|      0.0|     0.0|  1596494.0| (5,[4],[1596494.0])|(5,[4],[3.1597689...|\n",
      "|HNH619088|     0.0|    77275.0|  11361.0|     0.0|   917930.0|[0.0,77275.0,1136...|[0.0,0.0730445752...|\n",
      "|HNH036174|     0.0|    62674.0|      0.0|     0.0|   354879.0|(5,[1,4],[62674.0...|(5,[1,4],[0.05924...|\n",
      "|DNH067877|     0.0|        0.0|      0.0|     0.0|   181308.0|  (5,[4],[181308.0])|(5,[4],[0.3588434...|\n",
      "|SGH806190|     0.0|        0.0|      0.0|     0.0|   217779.0|  (5,[4],[217779.0])|(5,[4],[0.4310265...|\n",
      "|HNH582022|     0.0|        0.0|      0.0|     0.0|  2209949.0| (5,[4],[2209949.0])|(5,[4],[4.3739144...|\n",
      "|HNH795510|     0.0|    30197.0|    265.0|     0.0|  1196936.0|[0.0,30197.0,265....|[0.0,0.0285438633...|\n",
      "|DNFD91557|     0.0|        0.0|      0.0|     0.0|    95567.0|   (5,[4],[95567.0])|(5,[4],[0.1891454...|\n",
      "+---------+--------+-----------+---------+--------+-----------+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_scale_output.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette Score: 0.9999904478750545\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "silhouette_score=[]\n",
    "evaluator = ClusteringEvaluator(predictionCol='prediction', featuresCol='standardized', metricName='silhouette', distanceMeasure='squaredEuclidean')\n",
    "\n",
    "for i in range(2,10):    \n",
    "    KMeans_algo = KMeans(featuresCol='standardized', k=i)\n",
    "    KMeans_fit = KMeans_algo.fit(data_scale_output)\n",
    "    output = KMeans_fit.transform(data_scale_output)\n",
    "    score = evaluator.evaluate(output)\n",
    "    silhouette_score.append(score)\n",
    "    print(\"Silhouette Score:\",score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
